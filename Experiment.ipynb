{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.sparse import dok_matrix\n",
    "from numpy.random import choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length:  33924882 characters\n"
     ]
    }
   ],
   "source": [
    "f = open(\"FILES_TO_READ.txt\",\"r\")\n",
    "file_names = f.read()\n",
    "file_names = file_names.split()\n",
    "\n",
    "corpus = \"\"\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(\"data/\"+file_name, 'r') as f:\n",
    "            corpus+=f.read()\n",
    "corpus = corpus.replace('\\n',' ')\n",
    "corpus = corpus.replace('\\t',' ')\n",
    "corpus = corpus.replace('“', ' \" ')\n",
    "corpus = corpus.replace('”', ' \" ')\n",
    "for spaced in ['.','-',',','!','?','(','—',')']:\n",
    "    corpus = corpus.replace(spaced, ' {0} '.format(spaced))\n",
    "\n",
    "print(\"Corpus length: \",len(corpus),\"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in corpus:  6918692\n"
     ]
    }
   ],
   "source": [
    "corpus_words = corpus.split(' ')\n",
    "corpus_words= [word for word in corpus_words if word != '']\n",
    "print(\"Words in corpus: \",len(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words in corpus:  59014\n"
     ]
    }
   ],
   "source": [
    "distinct_words = list(set(corpus_words))\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "distinct_words_count = len(list(set(corpus_words)))\n",
    "print(\"Distinct words in corpus: \",distinct_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "sets_of_k_words = [ ' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k]) ]\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "sets_count = len(list(set(sets_of_k_words)))\n",
    "next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n",
    "\n",
    "distinct_sets_of_k_words = list(set(sets_of_k_words))\n",
    "k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "\n",
    "for i, word in enumerate(sets_of_k_words[:-k]):\n",
    "\n",
    "    word_sequence_idx = k_words_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "    next_after_k_words_matrix[word_sequence_idx, next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word_after_sequence(word_sequence, alpha):\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "    likelihoods = next_word_vector/next_word_vector.sum()\n",
    "    return choice(distinct_words, 1, p=likelihoods.toarray()[0])[0]\n",
    "    #return weighted_choice(distinct_words, likelihoods.toarray())\n",
    "\n",
    "def stochastic_chain(seed, chain_length=15, seed_length=2,alpha=0):\n",
    "    current_words = seed.split(' ')\n",
    "    if len(current_words) != seed_length:\n",
    "        print(len(current_words))\n",
    "        print(seed_length)\n",
    "        raise ValueError(f'wrong number of words, expected {seed_length}')\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(chain_length):\n",
    "        sentence+=' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words),alpha)\n",
    "        sentence+=next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLINE_ON = [\"MATT:\",\"MARISHA:\",\"LAURA:\",\"SAM:\",\"ASHLEY:\",\"TALIESIN:\",\"TRAVIS:\",\"LIAM:\",\"ORION:\"]\n",
    "NOSPACE_ON = [\".\",\",\",\"'\",\"\\\"\",\":\",\";\",\"?\",\"!\",\"-\"]\n",
    "\n",
    "def format_out(string_out):\n",
    "    str_formatted = \"\"\n",
    "    for word in string_out.split():\n",
    "        if word in NLINE_ON:\n",
    "            str_formatted += \"\\n\\n\" + word\n",
    "        elif word in NOSPACE_ON:\n",
    "            str_formatted += word\n",
    "        else:\n",
    "            str_formatted += \" \" + word\n",
    "    return str_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Caleb is a speed reader it's no big deal.\n",
      "\n",
      "LIAM: Oh, you haven't seen in a while?\n",
      "\n",
      "LIAM: We need to get the gold?\n",
      "\n",
      "LAURA: I feel really rusty.\n",
      "\n",
      "LAURA: How much is it?\n",
      "\n",
      "MARISHA: Yeah!\n",
      "\n",
      "LAURA: Wah- wah.\n"
     ]
    }
   ],
   "source": [
    "seed = \"Caleb is a\"\n",
    "length = 45\n",
    "\n",
    "print(format_out(stochastic_chain(seed, chain_length=length,seed_length=len(seed.split()),alpha=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
